{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import calendar\n",
    "from string import ascii_lowercase\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "__all__ = ['tqdm', 'trange']\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def format_interval(t):\n",
    "    mins, s = divmod(int(t), 60)\n",
    "    h, m = divmod(mins, 60)\n",
    "    if h:\n",
    "        return '%d:%02d:%02d' % (h, m, s)\n",
    "    else:\n",
    "        return '%02d:%02d' % (m, s)\n",
    "\n",
    "\n",
    "def format_meter(n, total, elapsed):\n",
    "    # n - number of finished iterations\n",
    "    # total - total number of iterations, or None\n",
    "    # elapsed - number of seconds passed since start\n",
    "    if n > total:\n",
    "        total = None\n",
    "    \n",
    "    elapsed_str = format_interval(elapsed)\n",
    "    rate = '%5.2f' % (n / elapsed) if elapsed else '?'\n",
    "    \n",
    "    if total:\n",
    "        frac = float(n) / total\n",
    "        \n",
    "        N_BARS = 10\n",
    "        bar_length = int(frac*N_BARS)\n",
    "        bar = '#'*bar_length + '-'*(N_BARS-bar_length)\n",
    "        \n",
    "        percentage = '%3d%%' % (frac * 100)\n",
    "        \n",
    "        left_str = format_interval(elapsed / n * (total-n)) if n else '?'\n",
    "        \n",
    "        return '|%s| %d/%d %s [elapsed: %s left: %s, %s iters/sec]' % (\n",
    "            bar, n, total, percentage, elapsed_str, left_str, rate)\n",
    "    \n",
    "    else:\n",
    "        return '%d [elapsed: %s, %s iters/sec]' % (n, elapsed_str, rate)\n",
    "\n",
    "\n",
    "class StatusPrinter(object):\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.last_printed_len = 0\n",
    "    \n",
    "    def print_status(self, s):\n",
    "        self.file.write('\\r'+s+' '*max(self.last_printed_len-len(s), 0))\n",
    "        self.file.flush()\n",
    "        self.last_printed_len = len(s)\n",
    "\n",
    "\n",
    "def tqdm(iterable, desc='', total=None, leave=False, file=sys.stderr,\n",
    "         mininterval=0.5, miniters=1):\n",
    "    \"\"\"\n",
    "    Get an iterable object, and return an iterator which acts exactly like the\n",
    "    iterable, but prints a progress meter and updates it every time a value is\n",
    "    requested.\n",
    "    'desc' can contain a short string, describing the progress, that is added\n",
    "    in the beginning of the line.\n",
    "    'total' can give the number of expected iterations. If not given,\n",
    "    len(iterable) is used if it is defined.\n",
    "    'file' can be a file-like object to output the progress message to.\n",
    "    If leave is False, tqdm deletes its traces from screen after it has\n",
    "    finished iterating over all elements.\n",
    "    If less than mininterval seconds or miniters iterations have passed since\n",
    "    the last progress meter update, it is not updated again.\n",
    "    \"\"\"\n",
    "    if total is None:\n",
    "        try:\n",
    "            total = len(iterable)\n",
    "        except TypeError:\n",
    "            total = None\n",
    "    \n",
    "    prefix = desc+': ' if desc else ''\n",
    "    \n",
    "    sp = StatusPrinter(file)\n",
    "    sp.print_status(prefix + format_meter(0, total, 0))\n",
    "    \n",
    "    start_t = last_print_t = time.time()\n",
    "    last_print_n = 0\n",
    "    n = 0\n",
    "    for obj in iterable:\n",
    "        yield obj\n",
    "        # Now the object was created and processed, so we can print the meter.\n",
    "        n += 1\n",
    "        if n - last_print_n >= miniters:\n",
    "            # We check the counter first, to reduce the overhead of time.time()\n",
    "            cur_t = time.time()\n",
    "            if cur_t - last_print_t >= mininterval:\n",
    "                sp.print_status(prefix + format_meter(n, total, cur_t-start_t))\n",
    "                last_print_n = n\n",
    "                last_print_t = cur_t\n",
    "    \n",
    "    if not leave:\n",
    "        sp.print_status('')\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        if last_print_n < n:\n",
    "            cur_t = time.time()\n",
    "            sp.print_status(prefix + format_meter(n, total, cur_t-start_t))\n",
    "        file.write('\\n')\n",
    "\n",
    "\n",
    "def trange(*args, **kwargs):\n",
    "    \"\"\"A shortcut for writing tqdm(range()) on py3 or tqdm(xrange()) on py2\"\"\"\n",
    "    try:\n",
    "        f = xrange\n",
    "    except NameError:\n",
    "        f = range\n",
    "    \n",
    "    return tqdm(f(*args), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://basketball.realgm.com/nba/draft/past_drafts/1996',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/1997',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/1998',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/1999',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2000',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2001',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2002',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2003',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2004',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2005',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2006',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2007',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2008',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2009',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2010',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2011',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2012',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2013',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2014',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2015',\n",
       " 'http://basketball.realgm.com/nba/draft/past_drafts/2016']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = []\n",
    "def get_pages():\n",
    "    for i in range(1996, 2017):\n",
    "        pages.append(\"http://basketball.realgm.com/nba/draft/past_drafts/{}\".format(i))\n",
    "    return pages\n",
    "get_pages()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "|----------| 0/55   0% [elapsed: 00:00 left: ?, ? iters/sec]"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] The system cannot find the file specified: 'h'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-62621a37a492>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# get the html\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib.pyc\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, proxies, context)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib.pyc\u001b[0m in \u001b[0;36mopen_file\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    467\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_ftp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_local_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mopen_local_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib.pyc\u001b[0m in \u001b[0;36mopen_local_file\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocalname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m         \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[0mmodified\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformatdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musegmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] The system cannot find the file specified: 'h'"
     ]
    }
   ],
   "source": [
    "urls = []\n",
    "for element in tqdm(pages[0]):\n",
    "    html = urlopen(element)  # get the html\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    table = soup.find_all('table')\n",
    "    for stuff in table:\n",
    "        rows = stuff.find_all('tr')[1:]\n",
    "        for text in rows:\n",
    "            links = text.find('a')\n",
    "            print links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|#####-----| 11/21  52% [elapsed: 00:14 left: 00:12,  0.78 iters/sec]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-16534961a975>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# get the html\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib.pyc\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, proxies, context)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\urllib.pyc\u001b[0m in \u001b[0;36mopen_http\u001b[1;34m(self, url, data)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddheaders\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m         \u001b[0merrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetreply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merrcode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\httplib.pyc\u001b[0m in \u001b[0;36mgetreply\u001b[1;34m(self, buffering)\u001b[0m\n\u001b[0;32m   1209\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1211\u001b[1;33m                 \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1212\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m                 \u001b[1;31m#only add this keyword if non-default for compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\httplib.pyc\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self, buffering)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1136\u001b[1;33m             \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1137\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwill_close\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0m_UNKNOWN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CS_IDLE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\httplib.pyc\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\httplib.pyc\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;31m# Initialize with Simple-Response defaults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "urls = []\n",
    "for element in tqdm(pages):\n",
    "    html = urlopen(element)  # get the html\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    table = soup.find_all('table')\n",
    "    for stuff in table:\n",
    "        rows = stuff.find_all('tr')[1:]\n",
    "        for text in rows:\n",
    "            links = text.find('a').get('href')\n",
    "            urls.append(\"http://basketball.realgm.com{}\".format(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months = {\"Jan\": \"January\", \n",
    "          \"Feb\": \"February\", \n",
    "          \"Mar\": \"March\", \n",
    "          \"Apr\": \"April\", \n",
    "          \"May\": \"May\", \n",
    "          \"Jun\": \"June\", \n",
    "          \"Jul\": \"July\", \n",
    "          \"Aug\": \"August\",\n",
    "          \"Sep\": \"September\",\n",
    "          \"Oct\": \"October\",\n",
    "          \"Nov\": \"November\",\n",
    "          \"Dec\": \"December\"}      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_url = \"http://basketball.realgm.com/player/AJ-Price/Summary/1656\"\n",
    "html = urlopen(test_url) # get the html\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "searchtext = re.compile(r'NCAA Season Stats - Totals',re.IGNORECASE)\n",
    "foundtext = soup.find('h2',text=searchtext)\n",
    "table = foundtext.findNext('table')\n",
    "rows = table.find_all('tr')[0]\n",
    "column_headers = ([th.getText() for th in rows.find_all('th')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:157: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:158: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "url_data = []\n",
    "name_data = []\n",
    "pos_data = []\n",
    "ht_data = []\n",
    "wt_data = []\n",
    "birthday_data = []\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    html = urlopen(url)  # get the html\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    span = soup.find('span', class_=\"feature\")\n",
    "    if span:\n",
    "        pos = span.get_text()\n",
    "    else:\n",
    "        pass\n",
    "    h2 = soup.find('h2')\n",
    "    if h2:\n",
    "        nameposnum = h2.get_text()\n",
    "        sep = '#'\n",
    "        if sep in nameposnum:\n",
    "            pre = nameposnum.split(sep, 1)[0]\n",
    "            namepos = pre.strip()\n",
    "            def rreplace(s, old, new, occurrence):\n",
    "                li = s.rsplit(old, occurrence)\n",
    "                return new.join(li)\n",
    "            pre = rreplace(namepos, pos, '', 1)\n",
    "            name = pre.strip()\n",
    "        else:\n",
    "            def rreplace(s, old, new, occurrence):\n",
    "                li = s.rsplit(old, occurrence)\n",
    "                return new.join(li)\n",
    "            pre = rreplace(nameposnum, pos, '', 1)\n",
    "            name = pre.strip()\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    div = soup.find('div', class_='half-column-left')\n",
    "    if div:\n",
    "        searchtext = re.compile(r'Height:')\n",
    "        foundtext = div.find('strong', text=searchtext)\n",
    "        if foundtext:\n",
    "            p = foundtext.parent\n",
    "            heightweight = p.get_text()\n",
    "            sep2 = '('\n",
    "            post2 = heightweight.split(sep2, 1)[1]\n",
    "            sep3 = 'cm)'\n",
    "            ht = post2.split(sep3, 1)[0]\n",
    "            weight = heightweight[-6:]\n",
    "            sep4 = 'kg)'\n",
    "            wt = weight.split(sep4, 1)[0]\n",
    "            searchtext1 = re.compile(r'Born:')\n",
    "            foundtext1 = div.find('strong', text=searchtext1)\n",
    "            if foundtext1:\n",
    "                p1 = foundtext1.parent\n",
    "                born= p1.get_text()\n",
    "                sep5 = ':'\n",
    "                post5 = born.split(sep5, 1)[1]\n",
    "                born1 = post5.strip()\n",
    "                sep6 = ' '\n",
    "                month = born1.split(sep6, 1)[0]\n",
    "                month1 = months[month]\n",
    "                monthdict = dict((v,k) for k,v in enumerate(calendar.month_name))\n",
    "                value = monthdict[month1]\n",
    "                month_dec = (float(value) - 1)/12\n",
    "                sep7 = ' '\n",
    "                post7 = born1.split(sep7, 1)[1]\n",
    "                sep8 = ','\n",
    "                day = post7.split(sep8, 1)[0]\n",
    "                day_dec = (float(day) - 1)/365\n",
    "                yearplus = post7.split(sep8, 1)[1]\n",
    "                sep9 = ' ('\n",
    "                year_raw = yearplus.split(sep9, 1)[0]\n",
    "                year = year_raw.strip()\n",
    "                year_dec = float(year) + month_dec + day_dec\n",
    "                searchtext2 = re.compile(r'NCAA Season Stats - Totals')\n",
    "                foundtext2 = soup.find('h2', text=searchtext2)\n",
    "                if foundtext2:\n",
    "                    table = foundtext2.findNext('table')\n",
    "                    rows = table.find_all('tr')[1:]\n",
    "                    data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "                    for row in rows:\n",
    "                        url_data.append(url)\n",
    "                        name_data.append(name)\n",
    "                        pos_data.append(pos)\n",
    "                        ht_data.append(ht)\n",
    "                        wt_data.append(wt)\n",
    "                        birthday_data.append(year_dec)\n",
    "                else:\n",
    "                    pass\n",
    "#                 searchtext3 = re.compile(r'International Regular Season Stats - Totals')\n",
    "#                 foundtext3 = soup.find('h2', text=searchtext3)\n",
    "#                 if foundtext3:\n",
    "#                     table = foundtext3.findNext('table')\n",
    "#                     rows = table.find_all('tr')[1:]\n",
    "#                     data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "#                     for row in rows:\n",
    "#                         url_data.append(url)\n",
    "#                         name_data.append(name)\n",
    "#                         pos_data.append(pos)\n",
    "#                         ht_data.append(ht)\n",
    "#                         wt_data.append(wt)\n",
    "#                         birthday_data.append(year_dec)\n",
    "#                 else:\n",
    "#                     pass\n",
    "#                 searchtext4 = re.compile(r'FIBA Junior Team Events Stats')\n",
    "#                 foundtext4 = soup.find('h2', text=searchtext4)\n",
    "#                 if foundtext4:\n",
    "#                     table = foundtext4.findNext('table')\n",
    "#                     rows = table.find_all('tr')[1:]\n",
    "#                     data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "#                     for row in rows:\n",
    "#                         url_data.append(url)\n",
    "#                         name_data.append(name)\n",
    "#                         pos_data.append(pos)\n",
    "#                         ht_data.append(ht)\n",
    "#                         wt_data.append(wt)\n",
    "#                         birthday_data.append(year_dec)\n",
    "#                 else:\n",
    "#                     pass\n",
    "#                 searchtext5 = re.compile(r'FIBA Senior Team Events Stats')\n",
    "#                 foundtext5 = soup.find('h2', text=searchtext5)\n",
    "#                 if foundtext5:\n",
    "#                     table = foundtext5.findNext('table')\n",
    "#                     rows = table.find_all('tr')[1:]\n",
    "#                     data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "#                     for row in rows:\n",
    "#                         url_data.append(url)\n",
    "#                         name_data.append(name)\n",
    "#                         pos_data.append(pos)\n",
    "#                         ht_data.append(ht)\n",
    "#                         wt_data.append(wt)\n",
    "#                         birthday_data.append(year_dec)\n",
    "#                 else:\n",
    "#                     pass\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "  \n",
    "df = pd.DataFrame(data, columns=column_headers)\n",
    "df['Name'] = name_data\n",
    "df['Url'] = url_data\n",
    "df['Position'] = pos_data\n",
    "df['Height'] = ht_data\n",
    "df['Weight'] = wt_data\n",
    "df['Born'] = birthday_data\n",
    "df['League'] = 'NCAA'\n",
    "df1 = df[df['Season'] != 'AVERAGES']\n",
    "df2 = df1[df1['Season'] != 'TOTAL']\n",
    "df3 = df2[df2['Season'] != 'CAREER']\n",
    "# df2['Season'] = df2['Year']\n",
    "df3['Weight'] = df3['Weight'].str.replace('[^\\w\\s]','')\n",
    "df3['Season'] = df3['Season'].str.replace('[^\\w\\s]','')\n",
    "df3['Season'] = df3['Season'].map(lambda x: str(x)[:4])\n",
    "df3['Season'] = pd.DataFrame(df3['Season'], dtype='int')\n",
    "df3['Season'] += 1\n",
    "df4 = df3.loc[df3['MIN'] != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_url = \"http://basketball.realgm.com/player/Dragan-Bender/Summary/41582\"\n",
    "html = urlopen(test_url) # get the html\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "searchtext = re.compile(r'International Regular Season Stats - Totals',re.IGNORECASE)\n",
    "foundtext = soup.find('h2',text=searchtext)\n",
    "table = foundtext.findNext('table')\n",
    "rows = table.find_all('tr')[0]\n",
    "column_headers = ([th.getText() for th in rows.find_all('th')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "url_data = []\n",
    "name_data = []\n",
    "pos_data = []\n",
    "ht_data = []\n",
    "wt_data = []\n",
    "birthday_data = []\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    html = urlopen(url)  # get the html\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    span = soup.find('span', class_=\"feature\")\n",
    "    if span:\n",
    "        pos = span.get_text()\n",
    "    else:\n",
    "        pass\n",
    "    h2 = soup.find('h2')\n",
    "    if h2:\n",
    "        nameposnum = h2.get_text()\n",
    "        sep = '#'\n",
    "        if sep in nameposnum:\n",
    "            pre = nameposnum.split(sep, 1)[0]\n",
    "            namepos = pre.strip()\n",
    "            def rreplace(s, old, new, occurrence):\n",
    "                li = s.rsplit(old, occurrence)\n",
    "                return new.join(li)\n",
    "            pre = rreplace(namepos, pos, '', 1)\n",
    "            name = pre.strip()\n",
    "        else:\n",
    "            def rreplace(s, old, new, occurrence):\n",
    "                li = s.rsplit(old, occurrence)\n",
    "                return new.join(li)\n",
    "            pre = rreplace(nameposnum, pos, '', 1)\n",
    "            name = pre.strip()\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    div = soup.find('div', class_='half-column-left')\n",
    "    if div:\n",
    "        searchtext = re.compile(r'Height:')\n",
    "        foundtext = div.find('strong', text=searchtext)\n",
    "        if foundtext:\n",
    "            p = foundtext.parent\n",
    "            heightweight = p.get_text()\n",
    "            sep2 = '('\n",
    "            post2 = heightweight.split(sep2, 1)[1]\n",
    "            sep3 = 'cm)'\n",
    "            ht = post2.split(sep3, 1)[0]\n",
    "            weight = heightweight[-6:]\n",
    "            sep4 = 'kg)'\n",
    "            wt = weight.split(sep4, 1)[0]\n",
    "            searchtext1 = re.compile(r'Born:')\n",
    "            foundtext1 = div.find('strong', text=searchtext1)\n",
    "            if foundtext1:\n",
    "                p1 = foundtext1.parent\n",
    "                born= p1.get_text()\n",
    "                sep5 = ':'\n",
    "                post5 = born.split(sep5, 1)[1]\n",
    "                born1 = post5.strip()\n",
    "                sep6 = ' '\n",
    "                month = born1.split(sep6, 1)[0]\n",
    "                month1 = months[month]\n",
    "                monthdict = dict((v,k) for k,v in enumerate(calendar.month_name))\n",
    "                value = monthdict[month1]\n",
    "                month_dec = (float(value) - 1)/12\n",
    "                sep7 = ' '\n",
    "                post7 = born1.split(sep7, 1)[1]\n",
    "                sep8 = ','\n",
    "                day = post7.split(sep8, 1)[0]\n",
    "                day_dec = (float(day) - 1)/365\n",
    "                yearplus = post7.split(sep8, 1)[1]\n",
    "                sep9 = ' ('\n",
    "                year_raw = yearplus.split(sep9, 1)[0]\n",
    "                year = year_raw.strip()\n",
    "                year_dec = float(year) + month_dec + day_dec\n",
    "#                 searchtext2 = re.compile(r'NCAA Season Stats - Totals')\n",
    "#                 foundtext2 = soup.find('h2', text=searchtext2)\n",
    "#                 if foundtext2:\n",
    "#                     table = foundtext2.findNext('table')\n",
    "#                     rows = table.find_all('tr')[1:]\n",
    "#                     data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "#                     for row in rows:\n",
    "#                         url_data.append(url)\n",
    "#                         name_data.append(name)\n",
    "#                         pos_data.append(pos)\n",
    "#                         ht_data.append(ht)\n",
    "#                         wt_data.append(wt)\n",
    "#                         birthday_data.append(year_dec)\n",
    "#                 else:\n",
    "#                     pass\n",
    "                searchtext3 = re.compile(r'International Regular Season Stats - Totals')\n",
    "                foundtext3 = soup.find('h2', text=searchtext3)\n",
    "                if foundtext3:\n",
    "                    table = foundtext3.findNext('table')\n",
    "                    rows = table.find_all('tr')[1:]\n",
    "                    data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "                    for row in rows:\n",
    "                        url_data.append(url)\n",
    "                        name_data.append(name)\n",
    "                        pos_data.append(pos)\n",
    "                        ht_data.append(ht)\n",
    "                        wt_data.append(wt)\n",
    "                        birthday_data.append(year_dec)\n",
    "                else:\n",
    "                    pass\n",
    "#                 searchtext4 = re.compile(r'FIBA Junior Team Events Stats')\n",
    "#                 foundtext4 = soup.find('h2', text=searchtext4)\n",
    "#                 if foundtext4:\n",
    "#                     table = foundtext4.findNext('table')\n",
    "#                     rows = table.find_all('tr')[1:]\n",
    "#                     data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "#                     for row in rows:\n",
    "#                         url_data.append(url)\n",
    "#                         name_data.append(name)\n",
    "#                         pos_data.append(pos)\n",
    "#                         ht_data.append(ht)\n",
    "#                         wt_data.append(wt)\n",
    "#                         birthday_data.append(year_dec)\n",
    "#                 else:\n",
    "#                     pass\n",
    "#                 searchtext5 = re.compile(r'FIBA Senior Team Events Stats')\n",
    "#                 foundtext5 = soup.find('h2', text=searchtext5)\n",
    "#                 if foundtext5:\n",
    "#                     table = foundtext5.findNext('table')\n",
    "#                     rows = table.find_all('tr')[1:]\n",
    "#                     data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "#                     for row in rows:\n",
    "#                         url_data.append(url)\n",
    "#                         name_data.append(name)\n",
    "#                         pos_data.append(pos)\n",
    "#                         ht_data.append(ht)\n",
    "#                         wt_data.append(wt)\n",
    "#                         birthday_data.append(year_dec)\n",
    "#                 else:\n",
    "#                     pass\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "  \n",
    "df = pd.DataFrame(data, columns=column_headers)\n",
    "df['Name'] = name_data\n",
    "df['Url'] = url_data\n",
    "df['Position'] = pos_data\n",
    "df['Height'] = ht_data\n",
    "df['Weight'] = wt_data\n",
    "df['Born'] = birthday_data\n",
    "df1 = df[df['Team'] != 'All Teams']\n",
    "df2 = df1[df1['League'] != 'All Leagues']\n",
    "df2['Weight'] = df2['Weight'].str.replace('[^\\w\\s]','')\n",
    "df2['Season'] = df2['Season'].str.replace('[^\\w\\s]','')\n",
    "df2['Season'] = df2['Season'].map(lambda x: str(x)[:4])\n",
    "df2['Season'] = pd.DataFrame(df2['Season'], dtype='int')\n",
    "df2['Season'] += 1\n",
    "df5 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_url = \"http://basketball.realgm.com/player/Zoran-Dragic/Summary/24599\"\n",
    "html = urlopen(test_url) # get the html\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "searchtext = re.compile(r'FIBA Senior Team Events Stats',re.IGNORECASE)\n",
    "foundtext = soup.find('h2',text=searchtext)\n",
    "table = foundtext.findNext('table')\n",
    "rows = table.find_all('tr')[0]\n",
    "column_headers = ([th.getText() for th in rows.find_all('th')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:169: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:170: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Matt\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "url_data = []\n",
    "name_data = []\n",
    "pos_data = []\n",
    "ht_data = []\n",
    "wt_data = []\n",
    "birthday_data = []\n",
    "\n",
    "for url in tqdm(urls):\n",
    "    html = urlopen(url)  # get the html\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    span = soup.find('span', class_=\"feature\")\n",
    "    if span:\n",
    "        pos = span.get_text()\n",
    "    else:\n",
    "        pass\n",
    "    h2 = soup.find('h2')\n",
    "    if h2:\n",
    "        nameposnum = h2.get_text()\n",
    "        sep = '#'\n",
    "        if sep in nameposnum:\n",
    "            pre = nameposnum.split(sep, 1)[0]\n",
    "            namepos = pre.strip()\n",
    "            def rreplace(s, old, new, occurrence):\n",
    "                li = s.rsplit(old, occurrence)\n",
    "                return new.join(li)\n",
    "            pre = rreplace(namepos, pos, '', 1)\n",
    "            name = pre.strip()\n",
    "        else:\n",
    "            def rreplace(s, old, new, occurrence):\n",
    "                li = s.rsplit(old, occurrence)\n",
    "                return new.join(li)\n",
    "            pre = rreplace(nameposnum, pos, '', 1)\n",
    "            name = pre.strip()\n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    div = soup.find('div', class_='half-column-left')\n",
    "    if div:\n",
    "        searchtext = re.compile(r'Height:')\n",
    "        foundtext = div.find('strong', text=searchtext)\n",
    "        if foundtext:\n",
    "            p = foundtext.parent\n",
    "            heightweight = p.get_text()\n",
    "            sep2 = '('\n",
    "            post2 = heightweight.split(sep2, 1)[1]\n",
    "            sep3 = 'cm)'\n",
    "            ht = post2.split(sep3, 1)[0]\n",
    "            weight = heightweight[-6:]\n",
    "            sep4 = 'kg)'\n",
    "            wt = weight.split(sep4, 1)[0]\n",
    "            searchtext1 = re.compile(r'Born:')\n",
    "            foundtext1 = div.find('strong', text=searchtext1)\n",
    "            if foundtext1:\n",
    "                p1 = foundtext1.parent\n",
    "                born= p1.get_text()\n",
    "                sep5 = ':'\n",
    "                post5 = born.split(sep5, 1)[1]\n",
    "                born1 = post5.strip()\n",
    "                sep6 = ' '\n",
    "                month = born1.split(sep6, 1)[0]\n",
    "                month1 = months[month]\n",
    "                monthdict = dict((v,k) for k,v in enumerate(calendar.month_name))\n",
    "                value = monthdict[month1]\n",
    "                month_dec = (float(value) - 1)/12\n",
    "                sep7 = ' '\n",
    "                post7 = born1.split(sep7, 1)[1]\n",
    "                sep8 = ','\n",
    "                day = post7.split(sep8, 1)[0]\n",
    "                day_dec = (float(day) - 1)/365\n",
    "                yearplus = post7.split(sep8, 1)[1]\n",
    "                sep9 = ' ('\n",
    "                year_raw = yearplus.split(sep9, 1)[0]\n",
    "                year = year_raw.strip()\n",
    "                year_dec = float(year) + month_dec + day_dec\n",
    "#                 searchtext2 = re.compile(r'NCAA Season Stats - Totals')\n",
    "#                 foundtext2 = soup.find('h2', text=searchtext2)\n",
    "#                 if foundtext2:\n",
    "#                     table = foundtext2.findNext('table')\n",
    "#                     rows = table.find_all('tr')[1:]\n",
    "#                     data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "#                     for row in rows:\n",
    "#                         url_data.append(url)\n",
    "#                         name_data.append(name)\n",
    "#                         pos_data.append(pos)\n",
    "#                         ht_data.append(ht)\n",
    "#                         wt_data.append(wt)\n",
    "#                         birthday_data.append(year_dec)\n",
    "#                 else:\n",
    "#                     pass\n",
    "#                 searchtext3 = re.compile(r'International Regular Season Stats - Totals')\n",
    "#                 foundtext3 = soup.find('h2', text=searchtext3)\n",
    "#                 if foundtext3:\n",
    "#                     table = foundtext3.findNext('table')\n",
    "#                     rows = table.find_all('tr')[1:]\n",
    "#                     data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "#                     for row in rows:\n",
    "#                         url_data.append(url)\n",
    "#                         name_data.append(name)\n",
    "#                         pos_data.append(pos)\n",
    "#                         ht_data.append(ht)\n",
    "#                         wt_data.append(wt)\n",
    "#                         birthday_data.append(year_dec)\n",
    "#                 else:\n",
    "#                     pass\n",
    "                searchtext4 = re.compile(r'FIBA Junior Team Events Stats')\n",
    "                foundtext4 = soup.find('h2', text=searchtext4)\n",
    "                if foundtext4:\n",
    "                    table = foundtext4.findNext('table')\n",
    "                    rows = table.find_all('tr')[1:]\n",
    "                    data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "                    for row in rows:\n",
    "                        url_data.append(url)\n",
    "                        name_data.append(name)\n",
    "                        pos_data.append(pos)\n",
    "                        ht_data.append(ht)\n",
    "                        wt_data.append(wt)\n",
    "                        birthday_data.append(year_dec)\n",
    "                else:\n",
    "                    pass\n",
    "                searchtext5 = re.compile(r'FIBA Senior Team Events Stats')\n",
    "                foundtext5 = soup.find('h2', text=searchtext5)\n",
    "                if foundtext5:\n",
    "                    table = foundtext5.findNext('table')\n",
    "                    rows = table.find_all('tr')[1:]\n",
    "                    data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "                    for row in rows:\n",
    "                        url_data.append(url)\n",
    "                        name_data.append(name)\n",
    "                        pos_data.append(pos)\n",
    "                        ht_data.append(ht)\n",
    "                        wt_data.append(wt)\n",
    "                        birthday_data.append(year_dec)\n",
    "                else:\n",
    "                    pass\n",
    "                searchtext6 = re.compile(r'Non-FIBA Events')\n",
    "                foundtext6 = soup.find('h2', text=searchtext6)\n",
    "                if foundtext6:\n",
    "                    table = foundtext6.findNext('table')\n",
    "                    rows = table.find_all('tr')[1:]\n",
    "                    data += ([[td.getText() for td in rows[i].find_all('td')] for i in range(len(rows))])\n",
    "                    for row in rows:\n",
    "                        url_data.append(url)\n",
    "                        name_data.append(name)\n",
    "                        pos_data.append(pos)\n",
    "                        ht_data.append(ht)\n",
    "                        wt_data.append(wt)\n",
    "                        birthday_data.append(year_dec)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(data, columns=column_headers)\n",
    "df['Name'] = name_data\n",
    "df['Url'] = url_data\n",
    "df['Position'] = pos_data\n",
    "df['Height'] = ht_data\n",
    "df['Weight'] = wt_data\n",
    "df['Born'] = birthday_data\n",
    "df['League'] = df['Event']\n",
    "df1 = df[df['Year'] != 'AVERAGES']\n",
    "df2 = df1[df1['Year'] != 'TOTAL']\n",
    "df2['Season'] = df2['Year']\n",
    "df2['Weight'] = df2['Weight'].str.replace('[^\\w\\s]','')\n",
    "df2['Season'] = df2['Season'].str.replace('[^\\w\\s]','')\n",
    "df2['Season'] = df2['Season'].map(lambda x: str(x)[:4])\n",
    "df2['Season'] = pd.DataFrame(df2['Season'], dtype='int')\n",
    "df2['Season'] += 1\n",
    "df6 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df7 = pd.concat([df4, df5, df6])\n",
    "df7.to_csv('RealGMDraft.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11648"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7 = pd.DataFrame.from_csv('RealGMDraft.csv')\n",
    "len(df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3551\n"
     ]
    }
   ],
   "source": [
    "df7.head()\n",
    "df_NCAA = df7.loc[df7['League'] == 'NCAA']\n",
    "print len(df_NCAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_NCAA.to_csv('df_NCAA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
